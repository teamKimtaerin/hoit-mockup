import logging
from typing import Dict, Any, List, Optional

from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser

from app.core.config import settings
from app.schemas.chatbot import ChatMessage

logger = logging.getLogger(__name__)


class LangChainBedrockService:
    """LangChainì„ ì‚¬ìš©í•œ AWS Bedrock ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        """LangChain ChatBedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.llm = ChatBedrock(
                model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
                region_name=settings.aws_bedrock_region,
                credentials_profile_name=None,  # í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
                model_kwargs={
                    "temperature": 0.7,
                    "max_tokens": 1000,
                },
            )

            # ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”
            self.output_parser = StrOutputParser()

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (MotionTextEditor í‘œì¤€ ì ìš©)
            self.system_template = """ë‹¹ì‹ ì€ MotionText v2.0 JSONì„ RFC6902 JSON Patchë¡œ ìˆ˜ì •í•˜ëŠ” ì „ë¬¸ í¸ì§‘ê¸°ì…ë‹ˆë‹¤.

<role>
ì‚¬ìš©ìì˜ ìì—°ì–´ ì§€ì‹œë¥¼ ë°›ì•„ MotionText v2.0 JSONì„ RFC6902 JSON Patchë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.
</role>

<rules>
- ìµœì†Œ ë³€ê²½: ì§€ì‹œëœ ë¶€ë¶„ë§Œ ìˆ˜ì •
- ì²­í‚¹ í•„ìˆ˜: ì¶œë ¥ì´ 1800í† í° ì´ˆê³¼ì‹œ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
- ìˆœì„œ ë³´ì¥: ê° ì²­í¬ëŠ” ì´ì „ ì²­í¬ ì ìš© í›„ ì ìš© ê°€ëŠ¥í•´ì•¼ í•¨
- RFC6902 í‘œì¤€: ì •í™•í•œ JSON Patch í˜•ì‹ ì¤€ìˆ˜
</rules>

<schema_requirements>
- version: 2.0 ìœ ì§€
- timebase: unit="seconds", fpsëŠ” ìˆ«ì
- time_fields: [start, end] ì´ˆ ë°°ì—´
- eType: "group"|"text"|"image"|"video"ë§Œ í—ˆìš©
- node_id: ëª¨ë“  ë…¸ë“œëŠ” ê³ ìœ  id í•„ìš”
</schema_requirements>

<processing_steps>
1. ì‚¬ìš©ì ì§€ì‹œ íŒŒì‹±
2. ì˜í–¥ë°›ëŠ” JSON ê²½ë¡œ ì‹ë³„
3. JSON Patch ì—°ì‚° ìƒì„±
4. 1800í† í° ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹
5. í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
</processing_steps>

<common_patterns>
- text_change: {{"op": "replace", "path": "/cues/0/root/text", "value": "ìƒˆ í…ìŠ¤íŠ¸"}}
- time_adjustment: {{"op": "replace", "path": "/cues/0/displayTime", "value": [0, 10]}}
- plugin_add: {{"op": "add", "path": "/cues/0/root/pluginChain/-", "value": {{"pluginId": "fadein@2.0.0", "timeOffset": ["0%", "100%"], "params": {{"animationDuration": 1.0}}}}}}
- plugin_param_edit: {{"op": "replace", "path": "/cues/0/root/pluginChain/1/params/typingSpeed", "value": 0.1}}
- style_edit: {{"op": "replace", "path": "/cues/0/root/style/color", "value": "#ff0000"}}
- word_plugin_add: {{"op": "add", "path": "/cues/0/root/children/0/pluginChain/-", "value": {{"pluginId": "glow@2.0.0", "params": {{"color": "#00ffff", "intensity": 0.8}}}}}}
- angry_emotion: {{"op": "add", "path": "/cues/0/root/children/0/pluginChain/-", "value": {{"pluginId": "cwi-loud@2.0.0", "timeOffset": [0, 0], "params": {{"color": "#ff0000", "pulse": {{"scale": 2.15, "lift": 12}}, "tremble": {{"ampPx": 1.5, "freq": 12}}}}}}}}
- multi_word_animation: ì—¬ëŸ¬ ë‹¨ì–´ì— ë™ì¼í•œ ì• ë‹ˆë©”ì´ì…˜ ì ìš© ì‹œ ê° ë‹¨ì–´ë§ˆë‹¤ ê°œë³„ patch ìƒì„±
  [
    {{"op": "add", "path": "/cues/0/root/children/0/pluginChain/-", "value": {{"pluginId": "glow@2.0.0", "params": {{"color": "#00ffff", "intensity": 0.8}}}}}},
    {{"op": "add", "path": "/cues/0/root/children/1/pluginChain/-", "value": {{"pluginId": "glow@2.0.0", "params": {{"color": "#00ffff", "intensity": 0.8}}}}}},
    {{"op": "add", "path": "/cues/0/root/children/2/pluginChain/-", "value": {{"pluginId": "glow@2.0.0", "params": {{"color": "#00ffff", "intensity": 0.8}}}}}}
  ]
- multi_word_emotion: ì—¬ëŸ¬ ë‹¨ì–´ì— ê°ì • í‘œí˜„ ì ìš©
  [
    {{"op": "add", "path": "/cues/0/root/children/1/pluginChain/-", "value": {{"pluginId": "cwi-loud@2.0.0", "timeOffset": [0, 0], "params": {{"color": "#ff0000", "pulse": {{"scale": 2.15, "lift": 12}}, "tremble": {{"ampPx": 1.5, "freq": 12}}}}}}}},
    {{"op": "add", "path": "/cues/0/root/children/2/pluginChain/-", "value": {{"pluginId": "cwi-loud@2.0.0", "timeOffset": [0, 0], "params": {{"color": "#ff0000", "pulse": {{"scale": 2.15, "lift": 12}}, "tremble": {{"ampPx": 1.5, "freq": 12}}}}}}}}
  ]
</common_patterns>

<multi_word_handling>
ì—¬ëŸ¬ ë‹¨ì–´ ì„ íƒ ì‹œ ê·œì¹™:
1. ì‚¬ìš©ìê°€ "ì²« ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸ ë‹¨ì–´", "ëª¨ë“  ë‹¨ì–´", "ë‹¨ì–´ë“¤" ë“±ìœ¼ë¡œ ë³µìˆ˜ ë‹¨ì–´ë¥¼ ì–¸ê¸‰í•˜ë©´ ê° ë‹¨ì–´ë§ˆë‹¤ ê°œë³„ patch ìƒì„±
2. ë™ì¼í•œ ì• ë‹ˆë©”ì´ì…˜ì„ ì—¬ëŸ¬ ë‹¨ì–´ì— ì ìš©í•  ë•ŒëŠ” ê° children[N] ì¸ë±ìŠ¤ë³„ë¡œ ë³„ë„ì˜ add ì—°ì‚° ìˆ˜í–‰
3. ì˜ˆì‹œ: "ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ë‹¨ì–´ì— glow íš¨ê³¼" â†’ children[0]ê³¼ children[1]ì— ê°ê° glow í”ŒëŸ¬ê·¸ì¸ ì¶”ê°€
4. ë°˜ë“œì‹œ multi_word_animation íŒ¨í„´ ì°¸ì¡°í•˜ì—¬ ì—¬ëŸ¬ patch ìƒì„±
</multi_word_handling>

<output_format>
<summary>ì„ íƒì‚¬í•­: ì´ Nê°œ ì—°ì‚°, ì£¼ìš” ë³€ê²½ì‚¬í•­</summary>
<json_patch_chunk index="1" total="N" ops="K">
<![CDATA[
[
  {{"op": "replace", "path": "/cues/0/root/text", "value": "ìƒˆ í…ìŠ¤íŠ¸"}}
]
]]>
</json_patch_chunk>
<apply_order>1,2,3</apply_order>
</output_format>

ì¤‘ìš”: ì„¤ëª… ì—†ì´ summary, json_patch_chunk, apply_orderë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„± (MotionTextEditor í‘œì¤€)
            self.prompt_template = ChatPromptTemplate.from_messages(
                [
                    SystemMessagePromptTemplate.from_template(self.system_template),
                    HumanMessagePromptTemplate.from_template(
                        "<user_instruction>{input}</user_instruction>\n\n"
                        "<current_json>\n{scenario_data}\n</current_json>\n\n"
                        "ìœ„ì˜ MotionText v2.0 JSONì— ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì„ ì ìš©í•˜ì—¬ RFC6902 JSON Patchë¡œ ì¶œë ¥í•˜ì„¸ìš”."
                    ),
                ]
            )

            # ì²´ì¸ êµ¬ì„± (í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì„œ)
            self.chain = self.prompt_template | self.llm | self.output_parser

            logger.info(
                f"LangChain ChatBedrock initialized for region: {settings.aws_bedrock_region}"
            )

        except Exception as e:
            logger.error(f"Failed to initialize LangChain ChatBedrock: {e}")
            raise

    def _convert_chat_history_to_messages(
        self, chat_history: List[ChatMessage]
    ) -> List:
        """ChatMessage ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        messages = []

        # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨ (í† í° ì ˆì•½)
        recent_messages = chat_history[-6:] if len(chat_history) > 6 else chat_history

        for msg in recent_messages:
            if msg.sender == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.sender == "bot":
                messages.append(AIMessage(content=msg.content))

        return messages

    def invoke_claude_with_chain(
        self,
        prompt: str,
        conversation_history: Optional[List[ChatMessage]] = None,
        scenario_data: Optional[Dict[str, Any]] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        LangChain ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ Claude ëª¨ë¸ í˜¸ì¶œ

        Args:
            prompt: ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.0-1.0)

        Returns:
            Dict containing completion and metadata
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            logger.info(f"ğŸ” [DEBUG] Input prompt: '{prompt}'")
            logger.info(
                f"ğŸ” [DEBUG] Prompt ends with '!!': {prompt.strip().endswith('!!')}"
            )
            logger.info(f"ğŸ” [DEBUG] Scenario data present: {scenario_data is not None}")
            if scenario_data:
                logger.info(f"ğŸ” [DEBUG] Scenario data type: {type(scenario_data)}")
                logger.info(
                    f"ğŸ” [DEBUG] Scenario data keys: {list(scenario_data.keys()) if isinstance(scenario_data, dict) else 'not dict'}"
                )

            # ë°ëª¨ í”„ë¡¬í”„íŠ¸ ì²´í¬ ('!!'ë¡œ ëë‚˜ëŠ” ê²½ìš°)
            if prompt.strip().endswith("!!"):
                logger.info(
                    "ğŸ­ DEMO PROMPT DETECTED - generating demo response with Loud animation and red gradient"
                )
                return self._generate_demo_response(scenario_data, prompt)

            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            self.llm.model_kwargs.update(
                {
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                }
            )

            logger.info(
                f"Invoking Claude via LangChain: max_tokens={max_tokens}, temp={temperature}"
            )

            # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì§ì ‘ í¸ì§‘ ì²´ì¸ ì‚¬ìš©
            if scenario_data:
                logger.info(
                    "ğŸ¯ Scenario data detected - using DIRECT SUBTITLE EDIT CHAIN"
                )
                logger.info(
                    f"ğŸ“Š Scenario data size: {len(str(scenario_data))} characters"
                )
                logger.info(f"ğŸ’¬ User prompt: '{prompt}'")

                # MotionText v2.0 ìŠ¤í‚¤ë§ˆ ê²€ì¦
                validation_result = self._validate_motion_text_schema(scenario_data)
                if not validation_result["valid"]:
                    logger.warning(
                        f"âš ï¸ Schema validation issues: {validation_result['errors']}"
                    )
                if validation_result["warnings"]:
                    logger.info(f"ğŸ“ Schema warnings: {validation_result['warnings']}")

                try:
                    edit_result = self.create_direct_subtitle_edit_chain(
                        user_message=prompt,
                        scenario_data=scenario_data,
                        max_tokens=max_tokens,
                        temperature=temperature,
                    )

                    logger.info("âœ… Direct edit chain completed successfully")
                    logger.info(
                        f"ğŸ“ Edit result type: {edit_result.get('type', 'unknown')}"
                    )
                    logger.info(f"âœ¨ Edit success: {edit_result.get('success', False)}")

                    # í¸ì§‘ ê²°ê³¼ë¥¼ ê¸°ë³¸ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    return {
                        "completion": edit_result.get("explanation", "í¸ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."),
                        "stop_reason": "end_turn",
                        "usage": {
                            "input_tokens": len(prompt.split()),
                            "output_tokens": len(str(edit_result).split()),
                        },
                        "model_id": self.llm.model_id,
                        "langchain_used": True,
                        "edit_result": edit_result,  # ì‹¤ì œ í¸ì§‘ ê²°ê³¼ í¬í•¨
                        "json_patches": edit_result.get("patches", []),  # JSON patch ì •ë³´
                        "request_params": {
                            "max_tokens": max_tokens,
                            "temperature": temperature,
                            "prompt_length": len(prompt),
                            "has_scenario_data": True,
                        },
                    }
                except Exception as e:
                    logger.error(
                        f"âŒ Direct edit chain failed, falling back to standard chain: {e}"
                    )
                    logger.warning(
                        "ğŸ”„ Switching to standard chain processing with scenario context"
                    )
                    # í¸ì§‘ ì²´ì¸ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²´ì¸ìœ¼ë¡œ fallback

            # ê¸°ë³¸ ì²´ì¸ ì‚¬ìš© (ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì—†ê±°ë‚˜ í¸ì§‘ ì²´ì¸ ì‹¤íŒ¨ì‹œ)
            logger.info("ğŸ”§ Using STANDARD CHAIN processing")

            scenario_json = ""
            if scenario_data:
                try:
                    import json

                    scenario_json = json.dumps(
                        scenario_data, indent=2, ensure_ascii=False
                    )
                    logger.info(
                        f"Scenario data included: {len(scenario_json)} characters"
                    )
                except Exception as e:
                    logger.warning(f"Failed to serialize scenario data: {e}")
                    scenario_json = "ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            else:
                scenario_json = "ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì—†ìŒ"

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ê²½ìš° ë©”ëª¨ë¦¬ ì‚¬ìš©
            if conversation_history and len(conversation_history) > 0:
                # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                memory = ConversationBufferMemory(
                    memory_key="chat_history", return_messages=True
                )

                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                messages = self._convert_chat_history_to_messages(conversation_history)
                for message in messages:
                    if isinstance(message, HumanMessage):
                        memory.chat_memory.add_user_message(message.content)
                    elif isinstance(message, AIMessage):
                        memory.chat_memory.add_ai_message(message.content)

                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° í¬í•¨)
                history_prompt = ChatPromptTemplate.from_messages(
                    [
                        SystemMessagePromptTemplate.from_template(self.system_template),
                        *messages,
                        HumanMessagePromptTemplate.from_template(
                            "ì‚¬ìš©ì ìš”ì²­: {input}\n\n"
                            "í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ (ìë§‰ ë° ìŠ¤íƒ€ì¼ë§ ë°ì´í„°):\n"
                            "```json\n{scenario_data}\n```\n\n"
                            "ìœ„ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
                        ),
                    ]
                )

                # íˆìŠ¤í† ë¦¬ í¬í•¨ ì²´ì¸
                history_chain = history_prompt | self.llm | self.output_parser
                completion = history_chain.invoke(
                    {"input": prompt, "scenario_data": scenario_json}
                )
            else:
                # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ì™€ í•¨ê»˜ ë‹¨ìˆœ ì²´ì¸ ì‚¬ìš©
                completion = self.chain.invoke(
                    {"input": prompt, "scenario_data": scenario_json}
                )

            logger.info("LangChain Claude invocation successful")
            logger.debug(f"Response preview: {completion[:200]}...")

            # ì‘ë‹µ ê²°ê³¼ êµ¬ì„±
            result = {
                "completion": completion,
                "stop_reason": "end_turn",  # LangChainì—ì„œëŠ” ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’
                "usage": {
                    "input_tokens": len(prompt.split()),  # ê·¼ì‚¬ì¹˜
                    "output_tokens": len(completion.split()),  # ê·¼ì‚¬ì¹˜
                },
                "model_id": self.llm.model_id,
                "langchain_used": True,
                "request_params": {
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "prompt_length": len(prompt),
                    "history_length": len(conversation_history)
                    if conversation_history
                    else 0,
                },
            }

            return result

        except Exception as e:
            logger.error(f"LangChain Claude invocation failed: {e}")

            # ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
            if "credentials" in str(e).lower() or "access" in str(e).lower():
                raise Exception("AWS ìê²©ì¦ëª…ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "throttling" in str(e).lower() or "rate" in str(e).lower():
                raise Exception("API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            elif "validation" in str(e).lower():
                raise Exception("ìš”ì²­ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"LangChainì„ í†µí•œ Claude í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

    def test_connection(self) -> bool:
        """
        LangChainì„ í†µí•œ Bedrock ì—°ê²° í…ŒìŠ¤íŠ¸

        Returns:
            bool: ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            test_result = self.invoke_claude_with_chain(
                prompt="ì•ˆë…•í•˜ì„¸ìš”",
                max_tokens=50,
                temperature=0.1,
            )
            return "completion" in test_result and len(test_result["completion"]) > 0
        except Exception as e:
            logger.error(f"LangChain connection test failed: {e}")
            return False

    def create_multi_step_chain(
        self,
        steps: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ì²´ì¸ ì‹¤í–‰

        Args:
            steps: ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ [{"step": "1", "prompt": "..."}, ...]
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: ê° ë‹¨ê³„ë³„ ê²°ê³¼ì™€ ìµœì¢… ê²°ê³¼
        """
        try:
            results = {}
            accumulated_context = ""

            logger.info(f"Starting multi-step chain with {len(steps)} steps")

            for i, step_info in enumerate(steps, 1):
                step_name = step_info.get("step", f"step_{i}")
                step_prompt = step_info.get("prompt", "")

                # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
                if accumulated_context:
                    full_prompt = (
                        f"ì´ì „ ë‹¨ê³„ ê²°ê³¼:\n{accumulated_context}\n\ní˜„ì¬ ë‹¨ê³„: {step_prompt}"
                    )
                else:
                    full_prompt = step_prompt

                logger.info(f"Executing step {i}/{len(steps)}: {step_name}")

                # ê° ë‹¨ê³„ë³„ ì²´ì¸ ì‹¤í–‰
                step_result = self.invoke_claude_with_chain(
                    prompt=full_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )

                results[step_name] = {
                    "prompt": step_prompt,
                    "response": step_result["completion"],
                    "step_number": i,
                    "usage": step_result.get("usage", {}),
                }

                # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ê²°ê³¼ ëˆ„ì 
                accumulated_context += f"\n[{step_name}] {step_result['completion']}"

            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                "multi_step_results": results,
                "final_context": accumulated_context,
                "total_steps": len(steps),
                "langchain_used": True,
                "chain_type": "multi_step",
            }

            logger.info(
                f"Multi-step chain completed successfully with {len(steps)} steps"
            )
            return final_result

        except Exception as e:
            logger.error(f"Multi-step chain failed: {e}")
            raise Exception(f"ë‹¤ë‹¨ê³„ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_parallel_chain(
        self,
        parallel_prompts: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸

        Args:
            parallel_prompts: ë³‘ë ¬ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ë“¤ [{"name": "task1", "prompt": "..."}, ...]
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: ê° ë³‘ë ¬ ì‘ì—…ë³„ ê²°ê³¼
        """
        try:
            results = {}

            logger.info(f"Starting parallel chain with {len(parallel_prompts)} tasks")

            for task_info in parallel_prompts:
                task_name = task_info.get("name", f"task_{len(results) + 1}")
                task_prompt = task_info.get("prompt", "")

                logger.info(f"Executing parallel task: {task_name}")

                # ê° ì‘ì—…ë³„ ë…ë¦½ì ì¸ ì²´ì¸ ì‹¤í–‰
                task_result = self.invoke_claude_with_chain(
                    prompt=task_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )

                results[task_name] = {
                    "prompt": task_prompt,
                    "response": task_result["completion"],
                    "usage": task_result.get("usage", {}),
                    "model_id": task_result.get("model_id"),
                }

            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                "parallel_results": results,
                "total_tasks": len(parallel_prompts),
                "langchain_used": True,
                "chain_type": "parallel",
            }

            logger.info(
                f"Parallel chain completed successfully with {len(parallel_prompts)} tasks"
            )
            return final_result

        except Exception as e:
            logger.error(f"Parallel chain failed: {e}")
            raise Exception(f"ë³‘ë ¬ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_conditional_chain(
        self,
        initial_prompt: str,
        conditions: List[Dict[str, Any]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        ì¡°ê±´ë¶€ ì²´ì¸ ì‹¤í–‰ (ì²« ë²ˆì§¸ ì‘ë‹µì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •)

        Args:
            initial_prompt: ì´ˆê¸° í”„ë¡¬í”„íŠ¸
            conditions: ì¡°ê±´ë³„ ë‹¤ìŒ ë‹¨ê³„ë“¤ [{"condition_keyword": "í‚¤ì›Œë“œ", "next_prompt": "..."}, ...]
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: ì¡°ê±´ë¶€ ì‹¤í–‰ ê²°ê³¼
        """
        try:
            logger.info("Starting conditional chain")

            # 1ë‹¨ê³„: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
            initial_result = self.invoke_claude_with_chain(
                prompt=initial_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            initial_response = initial_result["completion"].lower()

            # 2ë‹¨ê³„: ì¡°ê±´ í™•ì¸ ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
            matched_condition = None
            for condition in conditions:
                condition_keyword = condition.get("condition_keyword", "").lower()
                if condition_keyword and condition_keyword in initial_response:
                    matched_condition = condition
                    break

            if matched_condition:
                logger.info(
                    f"Condition matched: {matched_condition.get('condition_keyword')}"
                )

                # ì¡°ê±´ì— ë§ëŠ” ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰
                next_prompt = matched_condition.get("next_prompt", "")
                context_prompt = (
                    f"ì´ì „ ì‘ë‹µ: {initial_result['completion']}\n\n{next_prompt}"
                )

                next_result = self.invoke_claude_with_chain(
                    prompt=context_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )

                final_result = {
                    "initial_step": {
                        "prompt": initial_prompt,
                        "response": initial_result["completion"],
                        "usage": initial_result.get("usage", {}),
                    },
                    "conditional_step": {
                        "matched_condition": matched_condition.get("condition_keyword"),
                        "prompt": next_prompt,
                        "response": next_result["completion"],
                        "usage": next_result.get("usage", {}),
                    },
                    "final_response": next_result["completion"],
                    "langchain_used": True,
                    "chain_type": "conditional",
                }
            else:
                logger.info("No condition matched, using initial response")
                final_result = {
                    "initial_step": {
                        "prompt": initial_prompt,
                        "response": initial_result["completion"],
                        "usage": initial_result.get("usage", {}),
                    },
                    "conditional_step": None,
                    "final_response": initial_result["completion"],
                    "langchain_used": True,
                    "chain_type": "conditional",
                }

            logger.info("Conditional chain completed successfully")
            return final_result

        except Exception as e:
            logger.error(f"Conditional chain failed: {e}")
            raise Exception(f"ì¡°ê±´ë¶€ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def invoke_claude_with_xml_request(
        self,
        xml_request: str,
        max_tokens: int = 2000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        í†µí•©ëœ XML êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ Claudeë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤.

        Args:
            xml_request: XML í˜•ì‹ì˜ ìš”ì²­ (<user_instruction> + <current_json>)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: íŒŒì‹±ëœ ì‘ë‹µ (completion, json_patches, has_scenario_edits ë“±)
        """
        try:
            logger.info("ğŸš€ Processing unified XML request")

            # Claude í˜¸ì¶œ
            result = self.invoke_claude_with_chain(
                prompt=xml_request,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            # XML ì‘ë‹µ íŒŒì‹±
            motion_result = self._parse_motion_text_editor_response(
                result["completion"]
            )

            if motion_result["success"] and "patches" in motion_result:
                # plugin í˜•ì‹ ë³€í™˜ ì ìš©
                transformed_patches = self._transform_json_patches(
                    motion_result["patches"]
                )

                # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ê²½ìš°
                return {
                    "completion": result["completion"],
                    "stop_reason": result["stop_reason"],
                    "usage": result.get("usage"),
                    "model_id": result.get("model_id"),
                    "langchain_used": True,
                    "json_patches": transformed_patches,
                    "has_scenario_edits": bool(transformed_patches),
                }
            else:
                # íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ì¼ë°˜ ëŒ€í™”ì¸ ê²½ìš°
                return {
                    "completion": result["completion"],
                    "stop_reason": result["stop_reason"],
                    "usage": result.get("usage"),
                    "model_id": result.get("model_id"),
                    "langchain_used": True,
                    "json_patches": [],
                    "has_scenario_edits": False,
                }

        except Exception as e:
            logger.error(f"XML request processing failed: {e}")
            raise Exception(f"XML ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    def create_subtitle_animation_chain(
        self,
        user_message: str,
        subtitle_json: Optional[Dict[str, Any]] = None,
        max_tokens: int = 1500,
        temperature: float = 0.3,
    ) -> Dict[str, Any]:
        """
        HOIT ìë§‰ ì• ë‹ˆë©”ì´ì…˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹í™”ëœ ìˆœì°¨ ì²´ì¸
        Prompt.mdì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ë”°ë¦„

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            subtitle_json: ê¸°ì¡´ ìë§‰ JSON ë°ì´í„° (ì„ íƒì‚¬í•­)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (ë‚®ì€ ê°’ìœ¼ë¡œ ì •í™•ì„± ìš°ì„ )

        Returns:
            Dict: ë‹¨ê³„ë³„ ì²˜ë¦¬ ê²°ê³¼ì™€ ìµœì¢… JSON patch
        """
        try:
            logger.info("Starting HOIT subtitle animation chain")

            # ë‹¨ê³„ 1: ë©”ì‹œì§€ ìœ í˜• ë¶„ë¥˜
            classification_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ìœ í˜•ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ë¶„ë¥˜ ê¸°ì¤€:
1. "simple_info" - ë‹¨ìˆœí•œ ì •ë³´ ìš”ì²­
2. "simple_edit" - ê°„ë‹¨í•œ ìë§‰ ìˆ˜ì • (ì˜¤íƒˆì, ë‹¨ì–´ ë³€ê²½ ë“±)
3. "animation_request" - ìë§‰ ì• ë‹ˆë©”ì´ì…˜ ìˆ˜ì •/ì¶”ê°€ ìš”ì²­

ì‘ë‹µ í˜•ì‹:
{{
    "classification": "ë¶„ë¥˜ê²°ê³¼",
    "confidence": 0.95,
    "reasoning": "ë¶„ë¥˜ ê·¼ê±°"
}}"""

            step1_result = self.invoke_claude_with_chain(
                prompt=classification_prompt,
                max_tokens=200,
                temperature=0.1,
            )

            logger.info("Step 1 completed: Message classification")

            # ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹± ì‹œë„
            logger.info(
                f"ğŸ” Raw AI classification response: {step1_result['completion']}"
            )

            try:
                import json

                classification_data = json.loads(step1_result["completion"])
                classification = classification_data.get(
                    "classification", "animation_request"
                )
                confidence = classification_data.get("confidence", "unknown")
                reasoning = classification_data.get(
                    "reasoning", "No reasoning provided"
                )

                logger.info(
                    f"âœ… JSON parsing successful - Classification: {classification}, Confidence: {confidence}"
                )
                logger.info(f"ğŸ“ AI reasoning: {reasoning}")

            except (json.JSONDecodeError, KeyError, TypeError) as e:
                logger.warning(
                    f"âŒ JSON parsing failed ({type(e).__name__}: {e}), falling back to keyword-based classification"
                )

                # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
                response_lower = step1_result["completion"].lower()
                logger.info(
                    f"ğŸ”¤ Analyzing keywords in lowercase response: '{response_lower[:200]}...'"
                )

                if "simple_info" in response_lower or "ì •ë³´" in response_lower:
                    classification = "simple_info"
                    logger.info("ğŸ¯ Keyword match: 'simple_info' or 'ì •ë³´' found")
                elif "simple_edit" in response_lower or "ê°„ë‹¨" in response_lower:
                    classification = "simple_edit"
                    logger.info("ğŸ¯ Keyword match: 'simple_edit' or 'ê°„ë‹¨' found")
                else:
                    classification = "animation_request"
                    logger.info("ğŸ¯ Default fallback: classified as 'animation_request'")

            logger.info(
                f"ğŸ·ï¸  FINAL CLASSIFICATION: '{classification}' for user message: '{user_message}')"
            )

            # ë‹¨ê³„ 2: ë¶„ë¥˜ì— ë”°ë¥¸ ì²˜ë¦¬
            logger.info(f"ğŸ”„ Processing classification: {classification}")

            if classification == "simple_info":
                logger.info(
                    "ğŸ“š Processing as SIMPLE_INFO request - providing general information"
                )
                # ë‹¨ìˆœ ì •ë³´ ìš”ì²­ - ë°”ë¡œ ì‘ë‹µ
                info_prompt = f"""HOIT ìë§‰ í¸ì§‘ ë„êµ¬ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {user_message}

HOIT ì£¼ìš” ê¸°ëŠ¥:
- ìë™ ìë§‰ ìƒì„±
- ì‹¤ì‹œê°„ ìë§‰ í¸ì§‘
- ë‹¤ì–‘í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼
- í™”ì ë¶„ë¦¬ ë° ê´€ë¦¬
- GPU ê°€ì† ë Œë”ë§
- ë“œë˜ê·¸ ì•¤ ë“œë¡­ í¸ì§‘

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

                final_result = self.invoke_claude_with_chain(
                    prompt=info_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )

                return {
                    "chain_type": "subtitle_animation",
                    "classification": "simple_info",
                    "steps": {
                        "classification": step1_result,
                        "final_response": final_result,
                    },
                    "final_response": final_result["completion"],
                    "json_patch": None,
                    "langchain_used": True,
                }

            elif classification == "simple_edit":
                logger.info(
                    "âœï¸  Processing as SIMPLE_EDIT request - modifying subtitle text/style"
                )
                # ê°„ë‹¨í•œ ìë§‰ ìˆ˜ì •
                if not subtitle_json:
                    return {
                        "chain_type": "subtitle_animation",
                        "classification": "simple_edit",
                        "error": "ìë§‰ JSON ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                        "langchain_used": True,
                    }

                edit_prompt = f"""ê¸°ì¡´ ìë§‰ JSONì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ì„ JSON patch í˜•íƒœë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {user_message}
ê¸°ì¡´ ìë§‰ JSON: {json.dumps(subtitle_json, ensure_ascii=False, indent=2)}

JSON patch í˜•íƒœë¡œ ìˆ˜ì •ì‚¬í•­ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê¸°ì¡´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{
    "patches": [
        {{"op": "replace", "path": "/clips/0/text", "value": "ìˆ˜ì •ëœ í…ìŠ¤íŠ¸"}},
        {{"op": "add", "path": "/clips/1/style/color", "value": "#FF0000"}}
    ],
    "summary": "ìˆ˜ì • ë‚´ìš© ìš”ì•½"
}}"""

                edit_result = self.invoke_claude_with_chain(
                    prompt=edit_prompt,
                    max_tokens=max_tokens,
                    temperature=0.1,
                )

                return {
                    "chain_type": "subtitle_animation",
                    "classification": "simple_edit",
                    "steps": {
                        "classification": step1_result,
                        "edit_result": edit_result,
                    },
                    "final_response": edit_result["completion"],
                    "json_patch": edit_result["completion"],
                    "langchain_used": True,
                }

            else:  # animation_request
                logger.info(
                    "ğŸ¬ Processing as ANIMATION_REQUEST - extracting animation category and generating effects"
                )
                # ë‹¨ê³„ 3: ì• ë‹ˆë©”ì´ì…˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì‹¤ì œ manifest ê¸°ë°˜)
                category_prompt = f"""ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ì‚¬ìš© ê°€ëŠ¥í•œ ì• ë‹ˆë©”ì´ì…˜ ì¹´í…Œê³ ë¦¬ (ì‹¤ì œ manifest ê¸°ë°˜):

1. **rotation@2.0.0** - 3D íšŒì „ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: rotationDegrees(90-720Â°), animationDuration(0.5-3s), axisX/Y/Z(boolean), perspective(200-1500px), staggerDelay(0-0.3s)

2. **fadein@2.0.0** - í˜ì´ë“œ ì¸ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: staggerDelay(0.02-0.5s), animationDuration(0.2-2s), startOpacity(0-0.5), scaleStart(0.5-1), ease(power1-3.out/back.out/elastic.out)

3. **typewriter@2.0.0** - íƒ€ì´í•‘ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: typingSpeed(0.02-0.2s), cursorBlink(boolean), cursorChar(string), showCursor(boolean), soundEffect(boolean)

4. **glow@2.0.0** - ê¸€ë¡œìš° íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: color(#hex), intensity(0-1), pulse(boolean), cycles(1-120)

5. **scalepop@2.0.0** - ìŠ¤ì¼€ì¼ íŒ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: popScale(1.1-3), animationDuration(0.5-2.5s), staggerDelay(0-0.3s), bounceStrength(0.1-2), colorPop(boolean)

6. **slideup@2.0.0** - ìŠ¬ë¼ì´ë“œ ì—… íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: slideDistance(10-100px), animationDuration(0.3-2s), staggerDelay(0-0.5s), easeType(power2.out/back.out/elastic.out/bounce.out), blurEffect(boolean)

7. **elastic@2.0.0** - íƒ„ì„± íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: bounceStrength(0.1-2), animationDuration(0.5-4s), staggerDelay(0-0.5s), startScale(0-1), overshoot(1-2)

8. **glitch@2.0.0** - ê¸€ë¦¬ì¹˜ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: glitchIntensity(1-20px), animationDuration(0.5-5s), glitchFrequency(0.1-1s), colorSeparation(boolean), noiseEffect(boolean)

9. **flames@2.0.0** - ë¶ˆê½ƒ íš¨ê³¼ (GIF ê¸°ë°˜)
   - íŒŒë¼ë¯¸í„°: baseOpacity(0-1), flicker(0-1), cycles(1-120)

10. **pulse@2.0.0** - í„ìŠ¤ íš¨ê³¼
    - íŒŒë¼ë¯¸í„°: maxScale(1-2.5), cycles(0-10)

ë§Œì•½ ì¹´í…Œê³ ë¦¬ê°€ ëª…í™•í•˜ì§€ ì•Šë‹¤ë©´ ì‚¬ìš©ìì—ê²Œ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{
    "category": "ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ëª…",
    "confidence": 0.85,
    "suggested_parameters": {{
        "animationDuration": 1.5,
        "staggerDelay": 0.1
    }},
    "clarification_needed": false,
    "user_intent": "ì‚¬ìš©ì ì˜ë„ ë¶„ì„"
}}"""

                step3_result = self.invoke_claude_with_chain(
                    prompt=category_prompt,
                    max_tokens=300,
                    temperature=0.2,
                )

                # ë‹¨ê³„ 4: ì• ë‹ˆë©”ì´ì…˜ JSON ìƒì„± (manifest ìŠ¤í‚¤ë§ˆ ê¸°ë°˜)
                animation_prompt = f"""ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ manifest ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ì• ë‹ˆë©”ì´ì…˜ JSONì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼: {step3_result["completion"]}
ì‚¬ìš©ì ì›ë³¸ ìš”ì²­: {user_message}

ì‹¤ì œ manifest ìŠ¤í‚¤ë§ˆì— ë”°ë¼ JSON patchë¥¼ ìƒì„±í•˜ì„¸ìš”:

ì˜ˆì‹œ êµ¬ì¡°:
- **rotation@2.0.0**: {{"rotationDegrees": 360, "animationDuration": 1.5, "axisY": true, "perspective": 800, "staggerDelay": 0.1}}
- **fadein@2.0.0**: {{"staggerDelay": 0.1, "animationDuration": 0.8, "startOpacity": 0, "scaleStart": 0.9, "ease": "power2.out"}}
- **typewriter@2.0.0**: {{"typingSpeed": 0.05, "cursorBlink": true, "cursorChar": "|", "showCursor": true}}
- **glow@2.0.0**: {{"color": "#00ffff", "intensity": 0.4, "pulse": true, "cycles": 8}}
- **scalepop@2.0.0**: {{"popScale": 1.5, "animationDuration": 1.2, "staggerDelay": 0.08, "bounceStrength": 0.6, "colorPop": true}}
- **slideup@2.0.0**: {{"slideDistance": 30, "animationDuration": 1, "staggerDelay": 0.12, "easeType": "power2.out", "blurEffect": true}}
- **elastic@2.0.0**: {{"bounceStrength": 0.7, "animationDuration": 1.5, "staggerDelay": 0.1, "startScale": 0, "overshoot": 1.3}}
- **glitch@2.0.0**: {{"glitchIntensity": 5, "animationDuration": 2, "glitchFrequency": 0.3, "colorSeparation": true, "noiseEffect": true}}
- **flames@2.0.0**: {{"baseOpacity": 0.8, "flicker": 0.3, "cycles": 12}}
- **pulse@2.0.0**: {{"maxScale": 1.2, "cycles": 1}}

ì‘ë‹µ í˜•ì‹ (JSON patch):
{{
    "patches": [
        {{
            "op": "add",
            "path": "/clips/0/animation",
            "value": {{
                "plugin": "ì¹´í…Œê³ ë¦¬ëª…@2.0.0",
                "manifest": {{
                    "name": "ì¹´í…Œê³ ë¦¬ëª…",
                    "version": "2.0.0"
                }},
                "parameters": {{
                    "ì‹¤ì œìŠ¤í‚¤ë§ˆíŒŒë¼ë¯¸í„°ë“¤": "ì ì ˆí•œê°’"
                }}
            }}
        }}
    ],
    "animation_summary": "ì ìš©ëœ ì• ë‹ˆë©”ì´ì…˜ì˜ ìƒì„¸ ì„¤ëª…",
    "manifest_used": {{
        "plugin": "ì¹´í…Œê³ ë¦¬ëª…@2.0.0",
        "parameters_count": 5,
        "schema_validated": true
    }}
}}"""

                final_result = self.invoke_claude_with_chain(
                    prompt=animation_prompt,
                    max_tokens=max_tokens,
                    temperature=0.3,
                )

                return {
                    "chain_type": "subtitle_animation",
                    "classification": "animation_request",
                    "steps": {
                        "classification": step1_result,
                        "category_extraction": step3_result,
                        "animation_generation": final_result,
                    },
                    "final_response": final_result["completion"],
                    "json_patch": final_result["completion"],
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Subtitle animation chain failed: {e}")
            raise Exception(f"ìë§‰ ì• ë‹ˆë©”ì´ì…˜ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_direct_subtitle_edit_chain(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int = 1500,
        temperature: float = 0.1,
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ì²´ì¸

        Args:
            user_message: ì‚¬ìš©ì í¸ì§‘ ìš”ì²­
            scenario_data: í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ JSON ë°ì´í„°
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (ì •í™•ì„±ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •)

        Returns:
            Dict: JSON patchì™€ í¸ì§‘ ê²°ê³¼
        """
        try:
            logger.info("Starting direct subtitle edit chain")

            # 1ë‹¨ê³„: ìš”ì²­ ìœ í˜• ë¶„ë¥˜
            classification_prompt = f"""ì‚¬ìš©ìì˜ ìë§‰ í¸ì§‘ ìš”ì²­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: "{user_message}"

ë¶„ë¥˜ ê¸°ì¤€:
- "text_edit": ìë§‰ í…ìŠ¤íŠ¸ ìˆ˜ì • (ì˜¤íƒˆì, ë‹¨ì–´ ë³€ê²½)
- "style_edit": ìë§‰ ìŠ¤íƒ€ì¼ ìˆ˜ì • (ìƒ‰ìƒ, í¬ê¸°, ìœ„ì¹˜)
- "animation_request": ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ ì¶”ê°€/ìˆ˜ì •
- "info_request": ë‹¨ìˆœ ì •ë³´ ì§ˆë¬¸

JSON í˜•íƒœë¡œ ì‘ë‹µ:
{{"classification": "ë¶„ë¥˜ê²°ê³¼", "confidence": 0.95}}"""

            classification_result = self.invoke_claude_with_chain(
                prompt=classification_prompt,
                max_tokens=200,
                temperature=0.1,
            )

            logger.info(
                f"ğŸ” [DIRECT EDIT] Raw AI classification response: {classification_result['completion']}"
            )

            # ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹±
            try:
                import json

                classification_data = json.loads(classification_result["completion"])
                classification = classification_data.get("classification", "text_edit")
                confidence = classification_data.get("confidence", "unknown")

                logger.info(
                    f"âœ… [DIRECT EDIT] JSON parsing successful - Classification: {classification}, Confidence: {confidence}"
                )

            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(
                    f"âŒ [DIRECT EDIT] JSON parsing failed ({type(e).__name__}: {e}), using fallback classification"
                )
                classification = "text_edit"  # ê¸°ë³¸ê°’

            logger.info(
                f"ğŸ·ï¸ [DIRECT EDIT] FINAL CLASSIFICATION: '{classification}' for user message: '{user_message}'"
            )

            # 2ë‹¨ê³„: ë¶„ë¥˜ì— ë”°ë¥¸ í¸ì§‘ ì‹¤í–‰
            logger.info(f"ğŸ”„ [DIRECT EDIT] Dispatching to handler for: {classification}")

            if classification == "text_edit":
                logger.info("ğŸ“ [DIRECT EDIT] Calling text edit handler")
                return self._handle_text_edit(
                    user_message, scenario_data, max_tokens, temperature
                )
            elif classification == "style_edit":
                logger.info("ğŸ¨ [DIRECT EDIT] Calling style edit handler")
                return self._handle_style_edit(
                    user_message, scenario_data, max_tokens, temperature
                )
            elif classification == "animation_request":
                logger.info("ğŸ¬ [DIRECT EDIT] Calling animation request handler")
                return self._handle_animation_request(
                    user_message, scenario_data, max_tokens, temperature
                )
            else:
                logger.info("ğŸ“š [DIRECT EDIT] Calling info request handler")
                return self._handle_info_request(user_message, max_tokens, temperature)

        except Exception as e:
            logger.error(f"Direct subtitle edit chain failed: {e}")
            return {
                "type": "error",
                "error": f"í¸ì§‘ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_text_edit(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìˆ˜ì • ì²˜ë¦¬"""
        try:
            import json

            scenario_json = json.dumps(scenario_data, indent=2, ensure_ascii=False)

            edit_prompt = f"""<user_instruction>{user_message}</user_instruction>

<current_json>
{scenario_json}
</current_json>

ìœ„ì˜ MotionText v2.0 JSONì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”. RFC6902 JSON Patch í‘œì¤€ì„ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”."""

            result = self.invoke_claude_with_chain(
                prompt=edit_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            # MotionTextEditor ì‘ë‹µ íŒŒì‹± ì‹œë„
            motion_result = self._parse_motion_text_editor_response(
                result["completion"]
            )
            if motion_result["success"]:
                return motion_result

            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ì¡´ JSON í˜•ì‹ìœ¼ë¡œ fallback
            try:
                import json

                edit_result = json.loads(result["completion"])
                edit_result["langchain_used"] = True
                return edit_result
            except json.JSONDecodeError:
                return {
                    "type": "text_edit",
                    "success": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Text edit handling failed: {e}")
            return {
                "type": "text_edit",
                "error": f"í…ìŠ¤íŠ¸ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_style_edit(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """ìŠ¤íƒ€ì¼ ìˆ˜ì • ì²˜ë¦¬"""
        try:
            import json

            scenario_json = json.dumps(scenario_data, indent=2, ensure_ascii=False)

            style_prompt = f"""<user_instruction>{user_message}</user_instruction>

<current_json>
{scenario_json}
</current_json>

ìœ„ì˜ MotionText v2.0 JSONì—ì„œ ìŠ¤íƒ€ì¼ì„ ìˆ˜ì •í•˜ì„¸ìš”. RFC6902 JSON Patch í‘œì¤€ì„ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”."""

            result = self.invoke_claude_with_chain(
                prompt=style_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            # MotionTextEditor ì‘ë‹µ íŒŒì‹± ì‹œë„
            motion_result = self._parse_motion_text_editor_response(
                result["completion"]
            )
            if motion_result["success"]:
                return motion_result

            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ì¡´ JSON í˜•ì‹ìœ¼ë¡œ fallback
            try:
                import json

                edit_result = json.loads(result["completion"])
                edit_result["langchain_used"] = True
                return edit_result
            except json.JSONDecodeError:
                return {
                    "type": "style_edit",
                    "success": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Style edit handling failed: {e}")
            return {
                "type": "style_edit",
                "error": f"ìŠ¤íƒ€ì¼ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_animation_request(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """ì• ë‹ˆë©”ì´ì…˜ ìš”ì²­ ì²˜ë¦¬"""
        try:
            import json

            scenario_json = json.dumps(scenario_data, indent=2, ensure_ascii=False)

            animation_prompt = f"""<user_instruction>{user_message}</user_instruction>

<current_json>
{scenario_json}
</current_json>

ìœ„ì˜ MotionText v2.0 JSONì— ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ë¥¼ ì¶”ê°€í•˜ì„¸ìš”. RFC6902 JSON Patch í‘œì¤€ì„ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ì• ë‹ˆë©”ì´ì…˜:
- **bobY@2.0.0**: ìˆ˜ì§ ë°”ìš´ì‹± ì›€ì§ì„ (amplitudePx, cycles)
- **cwi-bouncing@2.0.0**: ë°”ìš´ì‹± ì›¨ì´ë¸Œ (speaker, palette, color, waveHeight)
- **cwi-color@2.0.0**: ìƒ‰ìƒ ì „í™˜ íš¨ê³¼ (speaker, palette, color, bulk)
- **cwi-loud@2.0.0**: í™”ë‚œ ëŠë‚Œ/í° ì†Œë¦¬ ì• ë‹ˆë©”ì´ì…˜ - ê°•ë ¬í•œ ê°ì • í‘œí˜„ì— ìµœì  (speaker, palette, color, pulse.scale, pulse.lift, tremble.ampPx, tremble.freq, timeOffset: [0,0])
- **cwi-whisper@2.0.0**: ì†ì‚­ì„ ì• ë‹ˆë©”ì´ì…˜ (speaker, palette, color, shrink.scale, shrink.drop, flutter.amp, flutter.freq)
- **elastic@2.0.0**: íƒ„ì„± ë°”ìš´ìŠ¤ íš¨ê³¼ (bounceStrength, animationDuration, staggerDelay, startScale, overshoot)
- **fadein@2.0.0**: í˜ì´ë“œì¸ ì• ë‹ˆë©”ì´ì…˜ (staggerDelay, animationDuration, startOpacity, scaleStart, ease)
- **flames@2.0.0**: ë¶ˆê½ƒ íš¨ê³¼ (baseOpacity, flicker, cycles)
- **fliptype@2.0.0**: í”Œë¦½ íƒ€ì´í•‘ ì• ë‹ˆë©”ì´ì…˜ (typingSpeed, flipDuration, flipAngle, flipDirection, typingDelay)
- **glitch@2.0.0**: ê¸€ë¦¬ì¹˜ íš¨ê³¼ (glitchIntensity, animationDuration, glitchFrequency, colorSeparation, noiseEffect)
- **glow@2.0.0**: ê¸€ë¡œìš° íš¨ê³¼ (color, intensity, pulse, cycles)
- **magnetic@2.0.0**: ìê¸° ëŒë¦¼ íš¨ê³¼ (magnetStrength, animationDuration, attractionDelay, elasticity)
- **pulse@2.0.0**: í„ìŠ¤ ì• ë‹ˆë©”ì´ì…˜ (maxScale, cycles)
- **rotation@2.0.0**: 3D íšŒì „ íš¨ê³¼ (rotationDegrees, animationDuration, staggerDelay, perspective, axisX, axisY, axisZ)
- **scalepop@2.0.0**: ìŠ¤ì¼€ì¼ íŒ íš¨ê³¼ (popScale, animationDuration, staggerDelay, bounceStrength, colorPop)
- **slideup@2.0.0**: ìŠ¬ë¼ì´ë“œì—… ì• ë‹ˆë©”ì´ì…˜ (slideDistance, animationDuration, staggerDelay, easeType, blurEffect)
- **spin@2.0.0**: ìŠ¤í•€ ì• ë‹ˆë©”ì´ì…˜ (fullTurns)
- **typewriter@2.0.0**: íƒ€ì´í”„ë¼ì´í„° íš¨ê³¼ (typingSpeed, cursorBlink, cursorChar, showCursor, soundEffect)"""

            result = self.invoke_claude_with_chain(
                prompt=animation_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            # MotionTextEditor ì‘ë‹µ íŒŒì‹± ì‹œë„
            motion_result = self._parse_motion_text_editor_response(
                result["completion"]
            )
            if motion_result["success"]:
                return motion_result

            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ì¡´ JSON í˜•ì‹ìœ¼ë¡œ fallback
            try:
                import json

                edit_result = json.loads(result["completion"])
                edit_result["langchain_used"] = True
                return edit_result
            except json.JSONDecodeError:
                return {
                    "type": "animation_request",
                    "success": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Animation request handling failed: {e}")
            return {
                "type": "animation_request",
                "error": f"ì• ë‹ˆë©”ì´ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_info_request(
        self, user_message: str, max_tokens: int, temperature: float
    ) -> Dict[str, Any]:
        """ì •ë³´ ìš”ì²­ ì²˜ë¦¬"""
        try:
            info_prompt = f"""ECG ìë§‰ í¸ì§‘ ë„êµ¬ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{user_message}"

ECG ì£¼ìš” ê¸°ëŠ¥:
- ìë™ ìë§‰ ìƒì„± ë° í¸ì§‘
- ë‹¤ì–‘í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼
- ì‹¤ì‹œê°„ ë¯¸ë¦¬ë³´ê¸°
- GPU ê°€ì† ë Œë”ë§

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            result = self.invoke_claude_with_chain(
                prompt=info_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            return {
                "type": "info_request",
                "success": True,
                "response": result["completion"],
                "langchain_used": True,
            }

        except Exception as e:
            logger.error(f"Info request handling failed: {e}")
            return {
                "type": "info_request",
                "error": f"ì •ë³´ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _generate_demo_response(
        self, scenario_data: Optional[Dict[str, Any]], prompt: str
    ) -> Dict[str, Any]:
        """ë°ëª¨ í”„ë¡¬í”„íŠ¸('!!'ë¡œ ëë‚˜ëŠ” ê²½ìš°) ì²˜ë¦¬ - ê¸°ì¡´ plugin chainì„ ë¹„ìš°ê³  Claudeê°€ ì±„ìš°ë„ë¡ í•¨"""
        try:
            logger.info(
                "ğŸ­ Demo mode: Clearing existing plugin chains and letting Claude fill them"
            )

            if not scenario_data or "cues" not in scenario_data:
                logger.warning("âš ï¸  No scenario data available for demo")
                # MotionTextEditor í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì—ëŸ¬ ì‘ë‹µ
                error_response = """<summary>ë°ëª¨ ëª¨ë“œ ì‹¤í–‰ ì‹¤íŒ¨, ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì—†ìŒ</summary>
<json_patch_chunk index="1" total="1" ops="0">
<![CDATA[
[]
]]>
</json_patch_chunk>
<apply_order>1</apply_order>"""
                return {
                    "completion": error_response,
                    "stop_reason": "end_turn",
                    "usage": {"input_tokens": len(prompt.split()), "output_tokens": 20},
                    "model_id": self.llm.model_id,
                    "langchain_used": True,
                    "edit_result": {
                        "type": "motion_text_edit",
                        "success": False,
                        "error": "No scenario data",
                    },
                    "json_patches": [],
                    "has_scenario_edits": False,
                }

            # Step 1: ê¸°ì¡´ plugin chain ì œê±°ë¥¼ ìœ„í•œ patches ìƒì„±
            import json
            import copy

            cleaned_scenario = copy.deepcopy(scenario_data)
            clear_patches = []

            logger.info("ğŸ§¹ Clearing existing plugin chains from scenario data")

            # ëª¨ë“  cueì™€ childrenì—ì„œ pluginChain ì œê±°
            for cue_index, cue in enumerate(cleaned_scenario.get("cues", [])):
                if "root" in cue:
                    root = cue["root"]

                    # rootì˜ pluginChain ì œê±°
                    if "pluginChain" in root:
                        clear_patches.append(
                            {
                                "op": "remove",
                                "path": f"/cues/{cue_index}/root/pluginChain",
                            }
                        )
                        del root["pluginChain"]

                    # childrenì˜ pluginChain ì œê±°
                    if "children" in root and isinstance(root["children"], list):
                        for child_index, child in enumerate(root["children"]):
                            if isinstance(child, dict) and "pluginChain" in child:
                                clear_patches.append(
                                    {
                                        "op": "remove",
                                        "path": f"/cues/{cue_index}/root/children/{child_index}/pluginChain",
                                    }
                                )
                                del child["pluginChain"]

            logger.info(
                f"ğŸ§¹ Generated {len(clear_patches)} clear patches for existing plugin chains"
            )

            # Step 2: Claudeì—ê²Œ plugin ì¶”ê°€ ìš”ì²­
            cleaned_scenario_json = json.dumps(
                cleaned_scenario, indent=2, ensure_ascii=False
            )

            demo_prompt = f"""<user_instruction>ë°ëª¨ ëª¨ë“œ: ëª¨ë“  ë‹¨ì–´ì™€ í…ìŠ¤íŠ¸ì— í™”ë‚œ ê°ì •ì„ í‘œí˜„í•˜ëŠ” ê°•ë ¬í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”. cwi-loud@2.0.0 ì• ë‹ˆë©”ì´ì…˜ê³¼ ë¶‰ì€ ìƒ‰ìƒ ê³„ì—´ì˜ glow íš¨ê³¼ë¥¼ ì ìš©í•˜ì—¬ ì—­ë™ì ì´ê³  ëˆˆì— ë„ëŠ” íš¨ê³¼ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.</user_instruction>

<current_json>
{cleaned_scenario_json}
</current_json>

ìœ„ì˜ MotionText v2.0 JSONì— ê°•ë ¬í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ë¥¼ ì¶”ê°€í•˜ì„¸ìš”. RFC6902 JSON Patch í‘œì¤€ì„ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”."""

            logger.info("ğŸ¤– Requesting Claude to add plugin chains for demo mode")

            result = self.invoke_claude_with_chain(
                prompt=demo_prompt,
                max_tokens=2000,
                temperature=0.3,
            )

            # Claude ì‘ë‹µ íŒŒì‹±
            motion_result = self._parse_motion_text_editor_response(
                result["completion"]
            )

            if motion_result["success"] and "patches" in motion_result:
                # plugin í˜•ì‹ ë³€í™˜ ì ìš©
                transformed_claude_patches = self._transform_json_patches(
                    motion_result["patches"]
                )

                # ê¸°ì¡´ clear_patchesì™€ ë³€í™˜ëœ Claude patches í•©ì¹˜ê¸°
                all_patches = clear_patches + transformed_claude_patches

                logger.info(
                    f"âœ… Demo completed: {len(clear_patches)} clear + {len(transformed_claude_patches)} Claude patches (transformed)"
                )

                return {
                    "completion": result["completion"],
                    "stop_reason": result["stop_reason"],
                    "usage": result.get(
                        "usage",
                        {
                            "input_tokens": len(demo_prompt.split()),
                            "output_tokens": 100,
                        },
                    ),
                    "model_id": result.get("model_id", self.llm.model_id),
                    "langchain_used": True,
                    "json_patches": all_patches,
                    "has_scenario_edits": True,
                    "demo_mode": True,
                }
            else:
                logger.warning("âš ï¸ Claude failed to generate demo patches")
                return {
                    "completion": "ë°ëª¨ ëª¨ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "stop_reason": "end_turn",
                    "usage": {
                        "input_tokens": len(demo_prompt.split()),
                        "output_tokens": 20,
                    },
                    "model_id": self.llm.model_id,
                    "langchain_used": True,
                    "json_patches": clear_patches,  # ìµœì†Œí•œ ê¸°ì¡´ ê²ƒì€ ì œê±°
                    "has_scenario_edits": True,
                    "demo_mode": True,
                }

        except Exception as e:
            logger.error(f"âŒ Demo response generation failed: {e}")
            import traceback

            logger.error(f"âŒ Full traceback: {traceback.format_exc()}")

            # MotionTextEditor í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì—ëŸ¬ ì‘ë‹µ
            error_response = """<summary>ë°ëª¨ ëª¨ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ</summary>
<json_patch_chunk index="1" total="1" ops="0">
<![CDATA[
[]
]]>
</json_patch_chunk>
<apply_order>1</apply_order>"""

            return {
                "completion": error_response,
                "stop_reason": "end_turn",
                "usage": {"input_tokens": len(prompt.split()), "output_tokens": 20},
                "model_id": self.llm.model_id,
                "langchain_used": True,
                "edit_result": {
                    "type": "motion_text_edit",
                    "success": False,
                    "error": str(e),
                },
                "json_patches": [],
                "has_scenario_edits": False,
                "demo_mode": True,
            }

    def _parse_motion_text_editor_response(self, response_text: str) -> Dict[str, Any]:
        """MotionTextEditor í‘œì¤€ ì‘ë‹µ íŒŒì‹± (CDATA í˜•ì‹ ë˜ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ ì§€ì›)"""
        try:
            import re
            import json

            logger.info("ğŸ” Parsing MotionTextEditor response format")

            # summary ì¶”ì¶œ
            summary_match = re.search(
                r"<summary>(.*?)</summary>", response_text, re.DOTALL
            )
            summary = summary_match.group(1).strip() if summary_match else ""

            # json_patch_chunk ì¶”ì¶œ
            chunk_pattern = r"<json_patch_chunk[^>]*>(.*?)</json_patch_chunk>"
            chunks = re.findall(chunk_pattern, response_text, re.DOTALL)

            # apply_order ì¶”ì¶œ
            order_match = re.search(
                r"<apply_order>(.*?)</apply_order>", response_text, re.DOTALL
            )
            apply_order = order_match.group(1).strip() if order_match else "1"

            all_patches = []

            for chunk_content in chunks:
                patch_json = ""

                # 1. CDATA í˜•ì‹ ì‹œë„
                cdata_match = re.search(
                    r"<!\[CDATA\[(.*?)\]\]>", chunk_content, re.DOTALL
                )
                if cdata_match:
                    patch_json = cdata_match.group(1).strip()
                    logger.debug("Found CDATA format JSON patch")
                else:
                    # 2. ì¼ë°˜ í…ìŠ¤íŠ¸ í˜•ì‹ ì‹œë„
                    patch_json = chunk_content.strip()
                    logger.debug("Using plain text format JSON patch")

                if patch_json:
                    try:
                        patches = json.loads(patch_json)
                        if isinstance(patches, list):
                            all_patches.extend(patches)
                        else:
                            all_patches.append(patches)
                        logger.debug(
                            f"Successfully parsed {len(patches) if isinstance(patches, list) else 1} patch(es)"
                        )
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON parsing failed for chunk: {e}")
                        logger.debug(f"Failed JSON content: {patch_json[:200]}...")

            logger.info(
                f"âœ… Parsed {len(all_patches)} patches from MotionTextEditor response"
            )

            # plugin í˜•ì‹ ë³€í™˜ ì ìš©
            transformed_patches = self._transform_json_patches(all_patches)

            return {
                "type": "motion_text_edit",
                "summary": summary or "MotionTextEditor í‘œì¤€ ì‘ë‹µ ì²˜ë¦¬ ì™„ë£Œ",
                "apply_order": apply_order,
                "success": len(transformed_patches) > 0,
                "langchain_used": True,
                "patches": transformed_patches,  # ë³€í™˜ëœ í˜•ì‹ìœ¼ë¡œ ì „ë‹¬
            }

        except Exception as e:
            logger.error(f"MotionTextEditor response parsing failed: {e}")
            logger.debug(f"Response text: {response_text[:500]}...")

            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ì¡´ JSON í˜•ì‹ìœ¼ë¡œ fallback ì‹œë„
            try:
                legacy_result = json.loads(response_text)
                if isinstance(legacy_result, dict) and "patches" in legacy_result:
                    return {
                        **legacy_result,
                        "langchain_used": True,
                        "fallback_parsing": True,
                    }
            except Exception as fallback_error:
                logger.debug(f"Legacy JSON parsing also failed: {fallback_error}")

            return {
                "type": "motion_text_edit",
                "summary": "",
                "success": False,
                "error": f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                "langchain_used": True,
            }

    def _transform_plugin_format(self, plugin_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Claudeê°€ ìƒì„±í•œ plugin ì •ë³´ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        From: {"pluginId": "cwi-loud@2.0.0", "timeOffset": [0, 0], "params": {...}}
        To: {"name": "cwi-loud", "params": {...}, "timeOffset": [...]}
        """
        try:
            # pluginIdì—ì„œ name ì¶”ì¶œ (ë²„ì „ ì œê±°)
            plugin_id = plugin_data.get("pluginId", "")
            name = plugin_id.split("@")[0] if "@" in plugin_id else plugin_id

            # ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            transformed = {
                "name": name,
                "params": plugin_data.get("params", {}),
                "timeOffset": plugin_data.get("timeOffset", [0, 0]),
            }

            logger.debug(f"ğŸ”„ Transformed plugin: {plugin_id} -> {name}")
            return transformed

        except Exception as e:
            logger.warning(f"âš ï¸ Plugin transformation failed: {e}")
            # fallback: ì›ë³¸ ë°ì´í„° ë°˜í™˜
            return plugin_data

    def _transform_json_patches(
        self, patches: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        JSON patchesì—ì„œ plugin ê´€ë ¨ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        try:
            transformed_patches = []

            for patch in patches:
                transformed_patch = patch.copy()

                # pluginChain ê´€ë ¨ patchì¸ì§€ í™•ì¸
                if (
                    patch.get("op") == "add"
                    and "pluginChain" in patch.get("path", "")
                    and "value" in patch
                ):
                    value = patch["value"]

                    # ë‹¨ì¼ plugin ê°ì²´ì¸ ê²½ìš°
                    if isinstance(value, dict) and "pluginId" in value:
                        transformed_patch["value"] = self._transform_plugin_format(
                            value
                        )

                    # plugin ë°°ì—´ì¸ ê²½ìš°
                    elif isinstance(value, list):
                        transformed_value = []
                        for item in value:
                            if isinstance(item, dict) and "pluginId" in item:
                                transformed_value.append(
                                    self._transform_plugin_format(item)
                                )
                            else:
                                transformed_value.append(item)
                        transformed_patch["value"] = transformed_value

                transformed_patches.append(transformed_patch)

            logger.info(
                f"ğŸ”„ Transformed {len(patches)} patches with plugin format conversion"
            )
            return transformed_patches

        except Exception as e:
            logger.error(f"âŒ JSON patch transformation failed: {e}")
            return patches  # fallback: ì›ë³¸ ë°˜í™˜

    def _estimate_token_count(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì  ê³„ì‚°)"""
        # ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì •: ë‹¨ì–´ ìˆ˜ * 1.3 (í•œêµ­ì–´/ì˜ì–´ í˜¼ì¬ ê³ ë ¤)
        return int(len(text.split()) * 1.3)

    def _should_chunk_response(self, response_text: str) -> bool:
        """ì‘ë‹µì´ ì²­í‚¹ì´ í•„ìš”í•œì§€ í™•ì¸ (1800í† í° ê¸°ì¤€)"""
        estimated_tokens = self._estimate_token_count(response_text)
        return estimated_tokens > 1800

    def _validate_motion_text_schema(
        self, scenario_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """MotionText v2.0 ìŠ¤í‚¤ë§ˆ ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        validation_result = {"valid": True, "errors": [], "warnings": []}

        try:
            # 1. version í™•ì¸
            if not scenario_data.get("version"):
                validation_result["errors"].append("Missing version field")
            elif scenario_data.get("version") != "2.0":
                validation_result["warnings"].append(
                    f"Version {scenario_data.get('version')} != 2.0"
                )

            # 2. timebase í™•ì¸
            timebase = scenario_data.get("timebase")
            if not timebase:
                validation_result["errors"].append("Missing timebase field")
            else:
                if not timebase.get("unit") == "seconds":
                    validation_result["errors"].append(
                        "timebase.unit must be 'seconds'"
                    )
                if not isinstance(timebase.get("fps"), (int, float)):
                    validation_result["errors"].append("timebase.fps must be a number")

            # 3. cues êµ¬ì¡° í™•ì¸
            cues = scenario_data.get("cues", [])
            if not isinstance(cues, list):
                validation_result["errors"].append("cues must be an array")
            else:
                for i, cue in enumerate(cues):
                    # displayTime í™•ì¸
                    display_time = cue.get("displayTime")
                    if not isinstance(display_time, list) or len(display_time) != 2:
                        validation_result["errors"].append(
                            f"cues[{i}].displayTime must be [start, end] array"
                        )

                    # eType í™•ì¸ (root ë…¸ë“œ)
                    root = cue.get("root", {})
                    if root.get("eType") not in ["group", "text", "image", "video"]:
                        validation_result["errors"].append(
                            f"cues[{i}].root.eType must be one of: group, text, image, video"
                        )

                    # node_id í™•ì¸
                    if not root.get("id"):
                        validation_result["warnings"].append(
                            f"cues[{i}].root missing id field"
                        )

            validation_result["valid"] = len(validation_result["errors"]) == 0

            if validation_result["valid"]:
                logger.info("âœ… MotionText v2.0 schema validation passed")
            else:
                logger.warning(
                    f"âš ï¸ Schema validation failed: {validation_result['errors']}"
                )

        except Exception as e:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Schema validation error: {str(e)}")
            logger.error(f"Schema validation exception: {e}")

        return validation_result


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
langchain_bedrock_service = LangChainBedrockService()
